{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import dmg.model2graph.model2graph as m2g\n",
    "import dmg.model2graph.metafilter as mf\n",
    "from networkx.algorithms.isomorphism import is_isomorphic\n",
    "import dmg.graphUtils as gu\n",
    "import glob\n",
    "from dmg.yakindu.yakinduPreprocess import removeLayout\n",
    "import dmg.yakindu.yakinduPallete as yp \n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metafilter_refs = ['Region.vertices', \n",
    "                           'CompositeElement.regions',\n",
    "                           'Vertex.outgoingTransitions',\n",
    "                           'Vertex.incomingTransitions',\n",
    "                           'Transition.target',\n",
    "                           'Transition.source']\n",
    "metafilter_cla = list(yp.dic_nodes_yak.keys())     \n",
    "metafilter_atts = None\n",
    "metafilterobj = mf.MetaFilter(references = metafilter_refs, \n",
    "                 attributes = metafilter_atts,\n",
    "                 classes = metafilter_cla)       \n",
    "meta_models = glob.glob(\"../data/metamodels/yakinduComplete/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../data/yakinduDataset/train/*\")\n",
    "graphs = []\n",
    "for f in files:\n",
    "    graphs.append(m2g.getGraphFromModel(f, \n",
    "                              meta_models, metafilterobj,\n",
    "                              consider_atts = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of graphs:', len(graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"../data/yakinduDataset/val/*\")\n",
    "graphs_val = []\n",
    "for f in files:\n",
    "    graphs_val.append(m2g.getGraphFromModel(f, \n",
    "                              meta_models, metafilterobj,\n",
    "                              consider_atts = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of graphs:', len(graphs_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "listDatas_val = []\n",
    "batch_size = 64\n",
    "print('Preparing seqs')\n",
    "for g in graphs_val:\n",
    "    sequence = yp.yakindu_pallete.graphToSequence(g)\n",
    "    listDatas_val = listDatas_val + sequence2data(sequence, yp.yakindu_pallete, max_len)\n",
    "loader_val = DataLoader(listDatas_val, batch_size=batch_size, \n",
    "                        num_workers = 0, \n",
    "                        shuffle=False)\n",
    "print('Seqs finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dmg.deeplearning.dataGeneration import sequence2data, data2graph\n",
    "from dmg.deeplearning.generativeModel import GenerativeModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "epochs = 100\n",
    "max_len = 2\n",
    "hidden_dim = 128\n",
    "\n",
    "criterion_node = nn.CrossEntropyLoss(reduction = 'mean',ignore_index=-1)\n",
    "criterion_action = nn.CrossEntropyLoss(reduction = 'mean')\n",
    "model = GenerativeModel(hidden_dim, yp.dic_nodes_yak, yp.dic_edges_yak, yp.dic_operations_yak)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    listDatas = []\n",
    "    #preparing training set\n",
    "    print('Preparing seqs')\n",
    "    for g in graphs:\n",
    "        sequence = yp.yakindu_pallete.graphToSequence(g)\n",
    "        listDatas = listDatas + sequence2data(sequence, yp.yakindu_pallete, max_len)\n",
    "    loader = DataLoader(listDatas, batch_size=batch_size, \n",
    "                            num_workers = 0, \n",
    "                            shuffle=False)\n",
    "    print('Seqs finished')\n",
    "    #training\n",
    "    for data in loader:\n",
    "        opt.zero_grad()\n",
    "        action, nodes = model(data.x, data.edge_index, \n",
    "                        torch.squeeze(data.edge_attr,dim=1), \n",
    "                data.batch, data.sequence, data.nodes, data.len_seq, data.action)\n",
    "        \n",
    "        nodes = torch.unsqueeze(nodes, dim = 2).repeat(1,1,2)\n",
    "        nodes[:,:,0] = 1 - nodes[:,:,1]\n",
    "            \n",
    "        L = torch.max(data.len_seq).item()\n",
    "        gTruth = data.sequence_masked[:,0:L]\n",
    "        loss = (criterion_node(nodes.reshape(-1,2), gTruth.flatten()) +\n",
    "                    criterion_action(action, data.action)) / 2\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    #validation\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in loader_val:\n",
    "            action, nodes = model(data.x, data.edge_index, \n",
    "                        torch.squeeze(data.edge_attr,dim=1), \n",
    "                data.batch, data.sequence, data.nodes, data.len_seq, data.action)\n",
    "            nodes = torch.unsqueeze(nodes, dim = 2).repeat(1,1,2)\n",
    "            nodes[:,:,0] = 1 - nodes[:,:,1]\n",
    "            \n",
    "            L = torch.max(data.len_seq).item()\n",
    "            gTruth = data.sequence_masked[:,0:L]\n",
    "            loss = (criterion_node(nodes.reshape(-1,2), gTruth.flatten()) +\n",
    "                    criterion_action(action, data.action)) / 2\n",
    "            val_loss+= loss.item()\n",
    "        \n",
    "    print('Epoch',epoch,'Loss Traning',total_loss/(len(loader)))\n",
    "    print('Epoch',epoch,'Loss Val',val_loss/(len(loader_val)))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
