{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import dmg.model2graph.model2graph as m2g\n",
    "import dmg.model2graph.metafilter as mf\n",
    "from networkx.algorithms.isomorphism import is_isomorphic\n",
    "import dmg.graphUtils as gu\n",
    "import glob\n",
    "from dmg.yakindu.yakinduPreprocess import removeLayout\n",
    "import dmg.yakindu.yakinduPallete as yp\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metafilter_refs = ['Region.vertices', \n",
    "                           'CompositeElement.regions',\n",
    "                           'Vertex.outgoingTransitions',\n",
    "                           'Vertex.incomingTransitions',\n",
    "                           'Transition.target',\n",
    "                           'Transition.source']\n",
    "metafilter_cla = list(yp.dic_nodes_yak.keys())     \n",
    "metafilter_atts = None\n",
    "metafilterobj = mf.MetaFilter(references = metafilter_refs, \n",
    "                 attributes = metafilter_atts,\n",
    "                 classes = metafilter_cla)       \n",
    "meta_models = glob.glob(\"../data/metamodels/yakinduComplete/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "files = glob.glob(\"data/yakinduDataset/train/*\")\n",
    "graphs = []\n",
    "for f in files:\n",
    "    graphs.append(m2g.model2graphJava('yakindu', f))\n",
    "    #m2g.getGraphFromModel(f, \n",
    "                                  #meta_models, metafilterobj,\n",
    "                                  #consider_atts = False)\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of graphs:', len(graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "files = glob.glob(\"data/yakinduDataset/val/*\")\n",
    "graphs_val = []\n",
    "for f in files:\n",
    "    #graphs_val.append(m2g.getGraphFromModel(f, \n",
    "    #                              meta_models, metafilterobj,\n",
    "    #                              consider_atts = False))\n",
    "    graphs_val.append(m2g.model2graphJava('yakindu', f))\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of graphs:', len(graphs_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp.yakindu_pallete.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from dmg.deeplearning.dataGeneration import sequence2data, data2graph\n",
    "from dmg.deeplearning.dataGeneration import addInvEdges\n",
    "\n",
    "listDatas_val = []\n",
    "batch_size = 64\n",
    "max_len = 2\n",
    "print('Preparing seqs')\n",
    "for g in graphs_val:\n",
    "    sequence = yp.yakindu_pallete.graphToSequence(g)\n",
    "    sequence = [(addInvEdges(s[0], yp.yakindu_pallete, yp.yakindu_separator),s[1]) for s in sequence]\n",
    "    listDatas_val = listDatas_val + sequence2data(sequence, yp.yakindu_pallete, max_len)\n",
    "loader_val = DataLoader(listDatas_val, batch_size=batch_size, \n",
    "                        num_workers = 0, \n",
    "                        shuffle=False)\n",
    "print('Seqs finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_eval = False\n",
    "\n",
    "if not do_eval:\n",
    "    graphs = graphs + graphs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of graphs:', len(graphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dmg.deeplearning.generativeModel import GenerativeModel\n",
    "from dmg.deeplearning.earlyStopping import EarlyStopping\n",
    "from dmg.yakindu.yakinduConsistency import inconsistent\n",
    "from dmg.deeplearning.generativeModel import sampleGraph\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "\n",
    "def f(g):\n",
    "    sequence = yp.yakindu_pallete.graphToSequence(g)\n",
    "    sequence = [(addInvEdges(s[0], yp.yakindu_pallete, yp.yakindu_separator),s[1]) for s in sequence]\n",
    "    return sequence2data(sequence, yp.yakindu_pallete, max_len)\n",
    "\n",
    "epochs = 100\n",
    "hidden_dim = 128\n",
    "\n",
    "\n",
    "criterion_node = nn.CrossEntropyLoss(reduction = 'mean',ignore_index=-1)\n",
    "criterion_action = nn.CrossEntropyLoss(reduction = 'mean')\n",
    "criterion_finish = nn.BCELoss(reduction = 'mean')\n",
    "model = GenerativeModel(hidden_dim, yp.dic_nodes_yak, yp.dic_edges_yak, yp.dic_operations_yak)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "es = EarlyStopping(opt, model, 'yakinduGenerator.model', patience = 10)\n",
    "prop_inconsistent = []\n",
    "prop_isomorfic = []\n",
    "prop_clean = []\n",
    "max_size = np.max([len(g) for g in graphs])\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    listDatas = []\n",
    "    #preparing training set\n",
    "    print('Preparing seqs')\n",
    "    #for g in graphs:\n",
    "    #    sequence = yp.yakindu_pallete.graphToSequence(g)\n",
    "    #    listDatas = listDatas + sequence2data(sequence, yp.yakindu_pallete, max_len)\n",
    "    with mp.Pool(10) as pool:\n",
    "        listDatas = pool.map(f, graphs)\n",
    "    listDatas = [r for rr in listDatas for r in rr]\n",
    "    print('Seqs finished')\n",
    "    loader = DataLoader(listDatas, batch_size=batch_size, \n",
    "                            num_workers = 0, \n",
    "                            shuffle=False)\n",
    "    #training\n",
    "    for data in loader:\n",
    "        opt.zero_grad()\n",
    "        action, nodes, finish = model(data.x, data.edge_index, \n",
    "                        torch.squeeze(data.edge_attr,dim=1), \n",
    "                data.batch, data.sequence, data.nodes, data.len_seq, data.action)\n",
    "        \n",
    "        nodes = torch.unsqueeze(nodes, dim = 2).repeat(1,1,2)\n",
    "        nodes[:,:,0] = 1 - nodes[:,:,1]\n",
    "            \n",
    "        L = torch.max(data.len_seq).item()\n",
    "        gTruth = data.sequence_masked[:,0:L]\n",
    "        loss = (criterion_node(nodes.reshape(-1,2), gTruth.flatten()) +\n",
    "                    criterion_action(action, data.action) +\n",
    "                    criterion_finish(finish.flatten(), data.finished.float())) / 3\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    #validation\n",
    "    if do_eval:\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in loader_val:\n",
    "                action, nodes, finish = model(data.x, data.edge_index, \n",
    "                            torch.squeeze(data.edge_attr,dim=1), \n",
    "                    data.batch, data.sequence, data.nodes, data.len_seq, data.action)\n",
    "                nodes = torch.unsqueeze(nodes, dim = 2).repeat(1,1,2)\n",
    "                nodes[:,:,0] = 1 - nodes[:,:,1]\n",
    "\n",
    "                L = torch.max(data.len_seq).item()\n",
    "                gTruth = data.sequence_masked[:,0:L]\n",
    "                loss = (criterion_node(nodes.reshape(-1,2), gTruth.flatten()) +\n",
    "                        criterion_action(action, data.action) +\n",
    "                        criterion_finish(finish.flatten(), data.finished.float())) / 3\n",
    "                val_loss+= loss.item()\n",
    "        \n",
    "    print('Epoch',epoch,'Loss Traning',total_loss/(len(loader)))\n",
    "    model.eval()\n",
    "    \n",
    "    samples = [sampleGraph(yp.G_initial_yak, yp.yakindu_pallete, model, max_size, yp.yakindu_separator) \n",
    "           for i in range(500)]\n",
    "    \n",
    "    inconsistents = []\n",
    "    for s in samples:\n",
    "        if inconsistent(s):\n",
    "            inconsistents.append(s)\n",
    "    inco_prop = len(inconsistents)*100/len(samples)\n",
    "    prop_inconsistent.append(inco_prop)\n",
    "    \n",
    "    iso = []\n",
    "    for s in samples:\n",
    "        for g in graphs:\n",
    "            if (is_isomorphic(s,g,gu.node_match_type, gu.edge_match_type)):\n",
    "                iso.append(s)\n",
    "                break\n",
    "    \n",
    "    iso_prop = len(iso)*100/len(samples)\n",
    "    prop_isomorfic.append(iso_prop)\n",
    "    \n",
    "    clean_new_models = [g for g in samples if (not g in inconsistents) and (not g in iso)]\n",
    "    clean_pr = len(clean_new_models)*100/len(samples)\n",
    "    prop_clean.append(clean_pr)\n",
    "    \n",
    "    print('Prop inconsistent:', inco_prop)\n",
    "    print('Prop isomorfic:', iso_prop)\n",
    "    print('Prop clean:', clean_pr)\n",
    "    \n",
    "    #scheduler.step()\n",
    "    #if do_eval:\n",
    "    #    print('Epoch',epoch,'Loss Val',val_loss/(len(loader_val)))\n",
    "    \n",
    "    if do_eval:\n",
    "        if es.step(val_loss/(len(loader_val)), epoch):\n",
    "            break\n",
    "    else:\n",
    "        if es.step(total_loss/(len(loader)), epoch):\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(prop_inconsistent, label = \"inconsistent\")\n",
    "plt.plot(prop_isomorfic, label = \"isomorfic\")\n",
    "plt.plot(prop_clean, label = \"clean\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "files = glob.glob(\"data/yakinduDataset/test/*\")\n",
    "graphs_test = []\n",
    "for f in files:\n",
    "    #graphs_test.append(m2g.getGraphFromModel(f, \n",
    "    #                              meta_models, metafilterobj,\n",
    "    #                              consider_atts = False))\n",
    "    graphs_test.append(m2g.model2graphJava('yakindu', f))\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmg.deeplearning.generativeModel import sampleGraph\n",
    "\n",
    "model.eval()\n",
    "samples = [sampleGraph(yp.G_initial_yak, yp.yakindu_pallete, model, 50, yp.yakindu_separator) \n",
    "           for i in range(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot([len(G) for G in samples], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([len(G) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dmg.realism.metrics as mt\n",
    "sns.distplot([np.mean(mt.getListDegree(G)) for G in samples], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([np.mean(mt.getListDegree(G)) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "wasserstein_distance([np.mean(mt.getListDegree(G)) for G in samples], \n",
    "                     [np.mean(mt.getListDegree(G)) for G in graphs_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = list(yp.dic_edges_yak.keys())\n",
    "sns.distplot([np.mean(list(mt.MPC(G,dims).values())) for G in samples], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([np.mean(list(mt.MPC(G,dims).values())) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')\n",
    "\n",
    "wasserstein_distance([np.mean(list(mt.MPC(G,dims).values())) for G in samples], \n",
    "                     [np.mean(list(mt.MPC(G,dims).values())) for G in graphs_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = list(yp.dic_edges_yak.keys())\n",
    "sns.distplot([np.mean(list(mt.nodeActivity(G,dims))) for G in samples], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([np.mean(list(mt.nodeActivity(G,dims))) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')\n",
    "\n",
    "wasserstein_distance([np.mean(list(mt.nodeActivity(G,dims))) for G in samples], \n",
    "                     [np.mean(list(mt.nodeActivity(G,dims))) for G in graphs_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check isomorf and consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = plt.hist([len(G) for G in samples], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.hist([len(G) for G in graphs], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check isomorf\n",
    "iso = []\n",
    "for s in samples:\n",
    "    for g in graphs:\n",
    "        if (is_isomorphic(s,g,gu.node_match_type, gu.edge_match_type)):\n",
    "            iso.append(s)\n",
    "            break\n",
    "print(len(iso)*100/len(samples),'% iso')\n",
    "not_iso = [g for g in samples if not g in iso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=[len(G) for G in iso])\n",
    "print('Mean size:', np.mean([len(G) for G in iso]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmg.yakindu.yakinduConsistency import inconsistent\n",
    "#check consistency\n",
    "inconsistents = []\n",
    "for s in samples:\n",
    "    if inconsistent(s):\n",
    "        inconsistents.append(s)\n",
    "print(len(inconsistents)*100/len(samples),'% inconsistents')\n",
    "not_inconsistents = [g for g in samples if not g in inconsistents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=[len(G) for G in inconsistents])\n",
    "print('Mean size:', np.mean([len(G) for G in inconsistents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_new_models = [g for g in not_iso if not g in inconsistents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=[len(G) for G in clean_new_models])\n",
    "print('Mean size:', np.mean([len(G) for G in clean_new_models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=[len(G) for G in graphs])\n",
    "print('Mean size:', np.mean([len(G) for G in graphs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_new_models),'clean models')\n",
    "print(len(clean_new_models)*100/len(samples),'% clean models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot([len(G) for G in clean_new_models], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([len(G) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot([np.mean(mt.getListDegree(G)) for G in clean_new_models], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([np.mean(mt.getListDegree(G)) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = list(yp.dic_edges_yak.keys())\n",
    "sns.distplot([np.mean(list(mt.MPC(G,dims).values())) for G in clean_new_models], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([np.mean(list(mt.MPC(G,dims).values())) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot([len(G) for G in not_inconsistents], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([len(G) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot([np.mean(mt.getListDegree(G)) for G in not_inconsistents], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([np.mean(mt.getListDegree(G)) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = list(yp.dic_edges_yak.keys())\n",
    "sns.distplot([np.mean(list(mt.MPC(G,dims).values())) for G in not_inconsistents], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'red', label = 'NN')\n",
    "sns.distplot([np.mean(list(mt.MPC(G,dims).values())) for G in graphs_test], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'blue', label = 'Real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniques(Gs):\n",
    "    dic = set([])\n",
    "    for G1 in Gs:\n",
    "        iso = False\n",
    "        for G2 in dic:\n",
    "            if is_isomorphic(G1, G2, gu.node_match_type, gu.edge_match_type):\n",
    "                iso = True\n",
    "        if not iso:\n",
    "            dic.add(G1)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(not_inconsistents)/len(samples) * 100, '% Validity among all')\n",
    "print(len(uniques(not_inconsistents))/len(not_inconsistents) * 100, '% Uniqueness among valid ones')\n",
    "print(len(uniques(clean_new_models))/len(uniques(samples)) * 100, '% Novelty among unique ones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realism using GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from dmg.deeplearning.dataGeneration import generateTensorsFromGraph\n",
    "syns = []\n",
    "sett = clean_new_models#samples#clean_new_models#not_inconsistents\n",
    "for G in random.sample(sett,min(len(sett),len(graphs_test))):\n",
    "    G_inv = addInvEdges(G, yp.yakindu_pallete, yp.yakindu_separator)\n",
    "    tensors = generateTensorsFromGraph(G_inv, yp.yakindu_pallete, 2, 2)\n",
    "    data =  Data(x = tensors[0],\n",
    "                edge_index = tensors[-2], \n",
    "                edge_attr = tensors[-1],\n",
    "                y = torch.tensor(0))\n",
    "    syns.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reals = []\n",
    "for G in random.sample(graphs_test,min(len(sett),len(graphs_test))):\n",
    "    G_inv = addInvEdges(G, yp.yakindu_pallete, yp.yakindu_separator)\n",
    "    tensors = generateTensorsFromGraph(G_inv, yp.yakindu_pallete, 2, 2)\n",
    "    data =  Data(x = tensors[0],\n",
    "                edge_index = tensors[-2], \n",
    "                edge_attr = tensors[-1],\n",
    "                y = torch.tensor(1))\n",
    "    reals.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = syns + reals\n",
    "random.shuffle(dataset)\n",
    "print('Len train:', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "train_len = int(0.8*len(dataset))\n",
    "test_len = len(dataset) - int(0.8*len(dataset))\n",
    "train, test = random_split(dataset, [train_len, test_len], \n",
    "                                generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=32, num_workers = 5, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=1, num_workers = 5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from dmg.realism.discriminativeModel import DiscriminativeModel\n",
    "\n",
    "model = DiscriminativeModel(64,64,0.0,yp.dic_nodes_yak,yp.dic_edges_yak).cpu()\n",
    "\n",
    "epochs = 30\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    b = 1\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        pred = model(data.x.cpu(), data.edge_index.cpu(),\n",
    "          torch.squeeze(data.edge_attr.cpu(),dim=1),data.batch.cpu())\n",
    "        \n",
    "        loss = criterion(torch.squeeze(pred), data.y.float().cpu())\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        b = b + 1\n",
    "        \n",
    "    print('Epoch',e,'Loss',total_loss/b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "count = 0\n",
    "i0 = 0\n",
    "i1 = 0\n",
    "for data in test_loader:\n",
    "    pred = model(data.x.cpu(), data.edge_index.cpu(),\n",
    "          torch.squeeze(data.edge_attr,dim=1).cpu(),data.batch.cpu())\n",
    "    if pred[0].item() > 0.5:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "    if pred == data.y.long().item():\n",
    "        count = count + 1\n",
    "    \n",
    "print('Acc', count/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import math\n",
    "\n",
    "def C2ST_pvalue(acc,n_test):\n",
    "    return st.norm.cdf(-(acc-0.5)/(math.sqrt(1/(4*n_test))))\n",
    "\n",
    "print('p-value', C2ST_pvalue(count/len(test_loader),len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot a sample of clean models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dot = gu.plotGraphViz(random.sample(clean_new_models,1)[0])\n",
    "dot.format = 'pdf'\n",
    "dot.view(filename='example', directory='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m2g.getModelFromGraph(['../data/metamodels/yakinduSimplified.ecore'], clean_new_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2g.serializeGraphModel('example.xmi',['../data/metamodels/yakinduSimplified.ecore'], 'Statechart', clean_new_models[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
